---
title: "Final Project"
author: "Congqi Lin & Weiting Yu"
date: "2022-12-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
rm(list = ls())
#Add libraries as needed
library(tidyverse)
library(FNN)
library(randomForest)
library(e1071)
```

```{r}
#Import datasets
Gamemodes <- read.csv("C:/Users/AndyL/OneDrive/桌面/22 FALL/STAT380/CODGameModes.csv")
Player1 <- read.csv("C:/Users/AndyL/OneDrive/桌面/22 FALL/STAT380/CODGames_p1_380.csv")
Player2 <- read.csv("C:/Users/AndyL/OneDrive/桌面/22 FALL/STAT380/CODGames_p2_380.csv")
```

## Task 1

```{r}
#Combine two datasets and change name of GameType
Allplayer <-
  Player1 %>%
  full_join(Player2)

Allplayer["GameType"][Allplayer["GameType"] == "HC - TDM"] <- "TDM"
Allplayer["GameType"][Allplayer["GameType"] == "HC - Kill Confirmed"] <- "Kill Confirmed"
Allplayer["GameType"][Allplayer["GameType"] == "HC - Hardpoint"] <- "Hardpoint"
Allplayer["GameType"][Allplayer["GameType"] == "HC - Domination"] <- "Domination"

```

```{r}
#Separate Result and add ScoreLimit
Allplayer2 <-
  Allplayer %>%
  separate(Result, c('ResultA','ResultB'),'-') %>%
  mutate(ResultA = as.numeric(ResultA),
         ResultB = as.numeric(ResultB)) %>%
  left_join(Gamemodes, by=c("GameType" = "Mode"))
```

```{r}
#Find all games reach score limit and calculate propotion.
ReachNum <-
  Allplayer2 %>%
  mutate(Reach = ifelse(ResultA == ScoreLimit, 1, ifelse(ResultB == ScoreLimit, 1, 0))) %>%
  filter(Reach == "1") %>%
  group_by(GameType) %>%
  summarize(reachnum = n())

TotalNum <-
  Allplayer2 %>%
  group_by(GameType) %>%
  summarize(totalnum = n()) %>%
  full_join(ReachNum) %>%
  mutate(propotion = reachnum / totalnum) %>%
  arrange(desc(propotion))

TotalNum

TotalNum %>%
  ggplot(aes(x = GameType, y = propotion)) +
  geom_bar(stat="identity", fill = "blue")

```

Based on the proportion and graph, the domination is most likely to reach the score limit.

## Task 2
To find the predictor associated with the TotalXP, we first check are there predictors with lots of missing value.
```{r}
colSums(is.na(Allplayer2))
```
By using is.na(), we find out that most of values are missing for Confirms, Denies, Objectives, ObjectiveKills, Captures, Diffuses, Plants, Detonates, Deposits, Time_Sec, and Time_Min. As a result, we don't consider them as our predictors. 

At the same time, we find out that there are some missing values in ResultA, ResultB, Eliminations, Deaths, Score, and Damage. We decide to remove all rows with these missing values. ResultA and ResultB actually represents whether the player win the game. As a result, we set new indicators called Win, Loss, and Draw based on the ResultA and ResultB.

In addition, even though the number of NA for categorical variable like Map1 and Map2 are 0, when we check the data, we see there are many blanks in these columns. Since these categorical variables have too many levels, which need to build a lot indicators for the model, we decide not to use Map1, Map2, Choice, MapVote, Date, and DidpPlayerVote as our predictors.
```{r}
#Select variables and creat new indicators
Allplayer3 <-
  Allplayer2 %>%
  select(TotalXP, FullPartial, ResultA, ResultB, Eliminations, Deaths, Score, Damage, XPType, GameType) %>%
  mutate(Win = ifelse(ResultA > ResultB, 1, 0),
         Loss = ifelse(ResultA < ResultB, 1, 0),
         Draw = ifelse(ResultA == ResultB, 1, 0)) %>%
  select(!ResultA) %>%
  select(!ResultB) %>%
  na.omit()
```

We try to build the model based on Allplayer3 dataset. However, we find that R automatically create 2 indicators for the XPType, which should only contain 2 levels. To solve this problem, we go back and check the level of XPType.

```{r}
#Check the level of XPTYpe
levels(as.factor(Allplayer3$XPType))
```

By using levels(), we find that there are three levels in total. There should be no difference between "Double XP + 10%" and "Double XP + 10% ". We think this may be a typo in the dataset so we decide to combine them together.

```{r}
#Fix the typo
Allplayer3["XPType"][Allplayer3["XPType"] == "Double XP + 10% "] <- "Double XP + 10%"
```

```{r}
#Build the multiple linear regression model
model1 <- lm(TotalXP ~ ., Allplayer3)
summary(model1)
#Backward elimination
step(model1, direction = ("backward"))
```
By using step() for the backward elimination, we find that Damage, Eliminations, Deaths, Loss, Score, XPType are the best predictors associated with the TotalXP because their AIC is the lowest.

```{r}
#Build the new multiple linear regression model based on the predictor we get
model2 <- lm(TotalXP ~ Damage + Eliminations + Deaths + Loss + Score + XPType, Allplayer3)
summary(model2)
```

For example, as the Eliminations increases or decreases by 1, the TotalXP will increase or decrease by 212.2143.

## Task 3
Research Question: Can we predict the game result based on player's performance during the game including Eliminations, Deaths, and Damage?

In this question, we will use Win as our indicator for game result and use Eliminations, Deaths, and Damage as our predictors. To solve this question, we are going to use kNN classfication, random forest, and SVM. kNN classfication is from the FNN package. It is used for estimating the likelihood that a data point will become a member of one group or another based on what group the data points nearest to it belong to. Random forest is from the randomForest package. It operates by constructing a multitude of decision trees at training time with the output of the class selected by most trees. SVM is from the e1071 package. It maps training examples to points in space so as to maximize the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.

As we learned in the class, whether the data is scaled would affect the model and result. As a result, we decide to build models seperatly with scaled and unscaled data. We start with the scaled data.

```{r}
#Scale the data and make the train/test split
xvars <- c("Eliminations", "Deaths", "Damage")
Allplayer4 <-
  Allplayer3
Allplayer4[ , xvars] <- scale(Allplayer4[ , xvars],
                      center = TRUE,
                      scale = TRUE)

set.seed(123)
train_ind <- sample(1:nrow(Allplayer4), floor(0.8 * nrow(Allplayer4)))
set.seed(NULL)

Train <- Allplayer4[train_ind, ]
Test <- Allplayer4[-train_ind, ]

```

Model 1: kNN classification
```{r}
#Find the best k
maxk <- 50
accuracy_vec <- rep(NA, maxk)
for(i in 2:maxk){
  knn_res_temp <- knn(train = Train[ , xvars, drop = FALSE],
                      test = Test[, xvars, drop = FALSE],
                      cl = Train$Win,
                      k = i)
  knn_res_temp[1:nrow(Test)]
  Testk <-
    Test %>%
    mutate(pred_win = knn_res_temp)
  accuracy_vec[i] = mean(Testk$pred_win == Testk$Win)
}
which.max(accuracy_vec)
```
```{r}
#Plot shows the different k with their accuracy
temp_df <- data.frame(k = 1:maxk, acc = accuracy_vec)
temp_df %>%
  ggplot(aes(x = k, y = acc)) +
  geom_point() +
  geom_line()
```

```{r}
#Build the kNN model based on the best k we found
knn_res <- knn(train = Train[ , xvars, drop = FALSE],
               test = Test[, xvars, drop = FALSE],
               cl = Train$Win,
               k = 28)
knn_res[1:nrow(Test)]
```
```{r}
#Calculate the accuracy for kNN model
Teste <-
  Test %>%
  mutate(pred_win = knn_res)
#Accuracy
mean(Teste$pred_win == Teste$Win)
```
Model 2: Random Forest

```{r}
#Build the random forest model with ntree = 500 and mtry = 3
rf <- randomForest(as.factor(Win) ~ Eliminations + Deaths + Damage, data = Train, ntree = 500, mtry = 3)
pred_prob <- predict(rf, newdata = Test, type = "prob")
pred_win <- predict(rf, newdata = Test, type = "response")
#Accuracy
mean(pred_win == Test$Win)
```
Model 3: SVM

```{r}
#Build the SVM model with cost = .1
svm <- svm(Win ~ Eliminations + Deaths + Damage, data = Train, kernel = "linear", cost = .1, type = "C-classification")
pred_prob_svm <- predict(svm, newdata = Test, type = "class")
#Accuracy
mean(pred_prob_svm == Test$Win)
```
Results: For the scaled data, kNN performs the best with an accuracy of 0.722. SVM has an accuracy of 0.698 and random forest has an accuracy of 0.673. These accuracy are not low, which means we can predict the game results by Elminations, Deaths, and Damage.

After that, we focus on unscaled data and try to compare the result.

```{r}
#Use the original data and do the train/test split
xvars <- c("Eliminations", "Deaths", "Damage")

set.seed(123)
train_ind2 <- sample(1:nrow(Allplayer3), floor(0.8 * nrow(Allplayer3)))
set.seed(NULL)

Train2 <- Allplayer3[train_ind2, ]
Test2 <- Allplayer3[-train_ind2, ]

```

Model 1: kNN classfication
```{r}
#Find the best k
maxk2 <- 50
accuracy_vec2 <- rep(NA, maxk2)
for(i in 2:maxk2){
  knn_res_temp2 <- knn(train = Train2[ , xvars, drop = FALSE],
                      test = Test2[, xvars, drop = FALSE],
                      cl = Train2$Win,
                      k = i)
  knn_res_temp2[1:nrow(Test2)]
  Testk2 <-
    Test2 %>%
    mutate(pred_win = knn_res_temp2)
  accuracy_vec2[i] = mean(Testk2$pred_win == Testk2$Win)
}
which.max(accuracy_vec2)
```
```{r}
#Plot shows the different k with their accuracy
temp_df2 <- data.frame(k = 1:maxk2, acc = accuracy_vec2)
temp_df2 %>%
  ggplot(aes(x = k, y = acc)) +
  geom_point() +
  geom_line()
```

```{r}
#Build the kNN model based on the best k we found

knn_res2 <- knn(train = Train2[ , xvars, drop = FALSE],
               test = Test2[, xvars, drop = FALSE],
               cl = Train2$Win,
               k = 3)
knn_res2[1:nrow(Test2)]
```
```{r}
#Calculate the accuracy for kNN model
Teste2 <-
  Test2 %>%
  mutate(pred_win = knn_res2)
#Accuracy
mean(Teste2$pred_win == Teste2$Win)
```
Model 2: Random Forest

```{r}
#Build the random forest model with ntree = 500 and mtry = 3
rf2 <- randomForest(as.factor(Win) ~ Eliminations + Deaths + Damage, data = Train2, ntree = 500, mtry = 3)
pred_prob2 <- predict(rf2, newdata = Test2, type = "prob")
pred_win2 <- predict(rf2, newdata = Test2, type = "response")
#Accuracy
mean(pred_win2 == Test2$Win)
```
Model 3: SVM

```{r}
#Build the SVM model with cost = 0.1
svm2 <- svm(Win ~ Eliminations + Deaths + Damage, data = Train2, kernel = "linear", cost = .1, type = "C-classification")
pred_prob_svm2 <- predict(svm2, newdata = Test2, type = "class")
#Accuracy
mean(pred_prob_svm2 == Test2$Win)
```
Results: Based on the result we got from the unscaled data, we find that the SVM performs the best with the highest accuracy of 0.698. The random forest has an accuracy of 0.636 and the kNN model has an accuracy of 0.586. By comparing the result from scaled and unscaled data, we find that both the accuracy for kNN and random forest is lower for the unscaled one. The accuracy of SVM keeps unchanged. This is because one of the default setting for the svm() is scale = TRUE, which means the function will automatically scale the data no matter our input is scale or not.

Conclusion: We can predict the game result based on player's Elimination, Deaths, and Damage by using kNN classfication, random forest, and svm. kNN classfication works the best with an accuracy of 0.722.